<div align="center">

# ğŸš€ Pathfinding Algorithms Comparison

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)]()
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow.svg)]()

</div>

This repository contains our group project for *COMP 9060 â€“ Applied Algorithm Analysis*, comparing classical and optimized pathfinding algorithms: **A\***, **Dijkstra**, **DFS**, and **JPS (Jump Point Search)**.  
The study focuses on performance, path optimality, and efficiency across different grid-based environments.


## ğŸ‘¥ Team Members
| Name | Role |
|------|------|
| **Yansong** | Algorithm Developer â€“ A*, Dijkstra, DFS Implementation |
| **Sepehr** | QA & Testing Lead â€“ JPS Implementation and Integration |
| **Vibhor** | Evaluation Lead â€“ Metrics Analysis and Visualization |
| **Binger** | Project Manager â€“ Documentation, Reporting, and Presentation |


## ğŸ¯ Project Overview
Pathfinding is a fundamental problem in AI, robotics, and game development.
This project aims to:
- Implement **A***, **Dijkstra**, **DFS**, and **JPS** algorithms in a common grid framework.
- Evaluate their performance on various **grid configurations** (sparse vs dense, small vs large).
- Measure **runtime**, **path cost**, and **node expansions** to analyze algorithmic efficiency.
- Visualize algorithm behavior through comparative charts and heatmaps.
**Jump Point Search (JPS)** improves **A*** by **skipping redundant nodes** in uniform-cost grids, reducing runtime while preserving optimal path cost.


## ğŸ—‚ï¸ Repository Structure
```
pathfinding-optimization/
â”œâ”€â”€ docs/                    # Documentation and reports
â”‚   â”œâ”€â”€ proposal.pdf         # Submitted project proposal
â”‚   â”œâ”€â”€ final_report.pdf     # Submitted final report
â”‚   â””â”€â”€ slides.pptx          # Presentation slides
â”‚
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for experiments and demos
â”‚   â””â”€â”€ final_grid_benchmark.ipynb   # Final grid benchmarking experiment
â”‚
â”œâ”€â”€ results/                 # Experiment outputs, logs, and performance data
â”‚   â”œâ”€â”€ figures/             # Generated charts and comparison graphs
â”‚   â””â”€â”€ logs/                # Raw runtime and node expansion logs
â”‚
â”œâ”€â”€ src/                     # Source code for all algorithms
â”‚   â””â”€â”€ algorithms/          # Pathfinding algorithm implementations
â”‚       â”œâ”€â”€ astar.py         # A* baseline algorithm
â”‚       â”œâ”€â”€ dfs.py           # Depth-First Search baseline
â”‚       â”œâ”€â”€ dijkstra.py      # Dijkstra baseline algorithm
â”‚       â”œâ”€â”€ jps.py           # Jump Point Search (JPS) implementation
â”‚       â””â”€â”€ mazegenerator.py # DFS-based random maze generator
â”‚   â””â”€â”€ core/                # Shared components
â”‚       â”œâ”€â”€ grid.py          # Grid representation and movement rules
â”‚       â””â”€â”€ utils.py         # Utility functions (logging, timers, helpers)
â”‚   â””â”€â”€ visualizations/      # Visualization and performance analysis
â”‚       â”œâ”€â”€ charts.py        # Static plots for paths and metrics
â”‚       â””â”€â”€ runtime_plot.py  # Search-progress / runtime-steps plots
â”‚   â””â”€â”€ main.py              # Entry point to run and compare algorithms
â”‚ 
â”œâ”€â”€ .gitignore               # Git ignore file
â”œâ”€â”€ LICENSE                  # Project license file
â”œâ”€â”€ README.md                # Project overview and usage instructions
â”œâ”€â”€ requirements.txt         # Python package dependencies
```

## âš™ï¸ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/bing-er/pathfinding-optimization.git
cd pathfinding-iptimization
```

### 2. Set Up Environment
```
python3 -m venv venv
source venv/bin/activate        # (Mac/Linux)
venv\Scripts\activate           # (Windows)
pip install -r requirements.txt
```

### 3. Run Algorithms
Run individual algorithms:
```
python src/algorithms/astar.py
python src/algorithms/dijkstra.py
python src/algorithms/dfs.py
python src/algorithms/jps.py
```
Or compare all from the main runner:
```
python src/main.py
```
### 4. Visualize Results
Generated logs and performance visualizations will appear in the results/ folder.
You can adjust grid size, obstacle density, or heuristic type in main.py.

### ğŸ“Š Evaluation Metrics

| Metric               | Description                      |
| -------------------- | -------------------------------- |
| **Path Length**      | Total distance of computed route |
| **Computation Time** | Time required to reach goal      |
| **Node Expansions**  | Number of explored nodes         |
| **Scalability**      | Performance on larger grid maps  |

## ğŸ§­ Progress Summary 
### Week 10 â€“ Midterm Status
By Week 10, our team has completed the baseline phase of the project. The core pathfinding algorithms - **A***, **Dijkstra**, and **DFS** - have all been implemented, tested, and merged into the main branch. We also added a maze generator to hlep us create consistent test grids for experiments. 

The repository is now fully organized with a clear folder structure, evaluation metrics, and documentation. Everyoneâ€™s roles are defined

ğŸ§  **Yansong** 
* Implemented and verified all baseline algorithms:
  * src/algorithms/astar.py
  * src/algorithms/dfs.py
  * src/algorithms/dijkstra.py
* Ensured path optimality and correctness for each baseline method.

âš™ï¸ **Sepehr**
* Fully implemented the Jump Point Search (JPS) algorithm:
  * src/algorithms/jps.py
* Added pruning, jump logic, and neighbor optimization for JPS.
* Supported debugging and alignment of JPS outputs with the baseline algorithms.
* Ensured the JPS module integrated cleanly with the main runner.

ğŸ“Š **Vibhor**
* Created the benchmark and visualization environment on Jupyter notebook:
  * Developed evaluation notebooks and scripts for algorithm comparison.
  * Designed performance testing plan (grid sizes, obstacle densities).
  * Set up visualization pipeline for runtime and node-expansion comparison.
* Generated early comparison plots and assisted in validating algorithm outputs.

ğŸ§© **Binger**
* Implemented core project infrastructure:
  * main.py â€” unified runner for all algorithms and comparison mode.
  * src/core/utils.py â€” grid utilities, timing, logging helpers.
  * src/visualizations/charts.py â€” plotting functions.
  * src/runtime_plot.py â€” runtime comparison script.
* Generated initial benchmark outputs:
  * results/figures/comparison.png
  * results/logs/runtime_log.csv
* Organized the folder structure, coordinated team workflow, and managed integration.

Our next milestone is to integrate and test **JPS**, comparing its performance against the baseline algorithms. The team will also begin logging runtime and node-expansion data and preparing visual outputs for comparison. In the following weeks, weâ€™ll move toward compiling the final report, creating visuals, and getting ready for our presentation in Week **14 (Dec 2)**.

### Week 11 - Transition to Performance Testing
By **Week 11 (Nov 11, 2025)**, our team completed the integration phase and transitioned into the **performance testing and visualization stage** of the Pathfinding Optimization Project.
All four pathfinding algorithms â€” **A***, **Dijkstra**, **DFS**, and **JPS** â€” are now unified under a consistent evaluation framework, allowing direct comparison on identical grid environments.

The full testing pipeline for **runtime**, **path length**, and **node-expansion metrics** has been finalized. The visualization layer is now being extended for clearer comparative analysis.

The repository is now fully operational, supporting **reproducible experiments**, **runtime logging**, and **benchmark visualizations**.
### âœ… Highlights (Week 11)
**Algorithm Integration & Framework**<br>
âœ”ï¸ All algorithms (A*, Dijkstra, DFS, JPS) integrated and verified under main.py --compare.<br>
âœ”ï¸ Unified output schema established for cross-algorithm comparison.<br>
âœ”ï¸ Consistent testing environment established using fixed random seeds.

**Benchmark & Testing Pipeline**<br>
* âœ”ï¸ Performance testing plan finalized
  * â†’ Grid sizes: 10Ã—10 â†’ 101Ã—101
  * â†’ Densities: 30%, 50%, 70%
* âœ”ï¸ Visualization notebooks updated for runtime and node-expansion comparison.
* âœ”ï¸ Benchmark suite (maze_benchmark_corners) integrated for comparative testing.

**Team Collaboration**<br>
* âœ”ï¸ Team meeting (Nov 11) to finalize responsibilities for runtime testing and data consolidation.
* âœ”ï¸ Visualization and logging pipeline now stable for batch testing.

#### ğŸ‘¥ Team Contributions
**ğŸ§  Yansong**
* Implemented and verified the **A***, **Dijkstra**, and **DFS** baseline algorithms.
* Validated path optimality and ensured consistent output formats.
* Assisted in testing alignment between all algorithm interfaces.

**âš™ï¸ Sepehr**
Finalized the **Jump Point Search (JPS)** algorithm with jump + pruning logic.
* Verified cross-comparison results between JPS and the baseline methods.
* Supported debugging and consistency checks across the benchmarking pipeline.

**ğŸ“Š Vibhor**
* Developed and pushed **benchmark testing notebooks and scripts** (`maze_benchmark_corners.ipynb`, `.py`, `.html`).
* Designed the **performance testing plan** for grid sizes and obstacle densities.
* Implemented **runtime and node-expansion visualization** in Jupyter Notebook.
* Coordinated with team for data collection and figure generation.

**ğŸ§© Binger**

* Implemented and maintained the **main runner** (`main.py`) with unified execution and `--compare` mode.
* Integrated Vibhorâ€™s visualization branch into `main` and verified functionality.
* Updated **logging and result management** for consistent output to `results/figures/` and `results/logs/`.
* Coordinated Week 11 progress and organized next-phase performance testing tasks.
* Added detailed comments and clarifications inside the benchmark notebook
* (final_grid_benchmark.ipynb) to improve readability, explain logic flow, and support team understanding.

## ğŸ“… Next Milestones
### Pefrformance Testing (Nov 15 - Nov 22)
Conduct batch tests on grid sizes **31Ã—31**, **61Ã—61**, and **91Ã—91**.
* Collect:
  * Runtime measurements
  * Path length results
  * Node-expansion metrics
  * across all algorithms.
* Finalize plotting and comparison results for the final report.
### Final Deliverables (Nov 22 â€“ Dec 2)
* Integrate comparison plots into the final paper.
* Begin drafting:
  * Final Report
  * Presentation Slides (Team 3)

## ğŸ“… Updated Project Timeline

| **Milestone** | **Due Date** | **Status** |
|----------------|--------------|-------------|
| Proposal Submission | Oct 21, 2025 | âœ… Submitted |
| Implementation Phase (A*, Dijkstra, DFS, JPS) | Nov 8, 2025 | âœ… Completed |
| Performance Testing + Visualization | Nov 18, 2025 | âœ… Completed |
| Final Report & Presentation | Dec 2, 2025 | âœ… Completed |


## ğŸ“Š Additional Visualization

<img width="600" alt="RuntimeComparison" src="results/figures/fig_runtime91.png">

<img width="600" alt="RuntimeComparison" src="results/figures/fig_subopt.png">

<img width="600" alt="RuntimeComparison" src="results/figures/fig_ecdf.png">

<img width="600" alt="RuntimeComparison" src="results/figures/fig_scaling.png">

<img width="600" alt="RuntimeComparison" src="results/figures/comparison_final.png">

## ğŸ“œ License

This project is developed for **educational purposes** under the **BCIT COMP 9060 â€“ Applied Algorithm Analysis** course.  
Licensed under the [MIT License](LICENSE).


### ğŸ”— **Useful Links**

- ğŸ“˜ [Overleaf Proposal](https://www.overleaf.com/9465635879vhhjjwjkmhzk#37ad93)  
- ğŸ“„ [Overleaf Final Report](https://www.overleaf.com/6623247675ghmpxqtkrbhc#20506f)  
- ğŸ—‚ï¸ [GitHub Project Board](https://github.com/yourusername/COMP9060-Pathfinding-Optimization/projects)  
- ğŸ“Š [Results Dashboard (optional)](https://colab.research.google.com/drive/your-dashboard-link)
